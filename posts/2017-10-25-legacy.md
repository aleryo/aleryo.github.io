---
title: Ils ne savaient pas que c’était impossible...
author: Aleryo
---

# Votre mission, si vous l'acceptez…

## L'agence tous risques

Tout commence comme un banal appel d'offres pour la reprise en tierce maintenance applicative d'une application d'un éditeur dans le domaine de la finance : le client cherche une équipe avec un PO, un testeur, 1 développeur sénior et 2 juniors. L'application a une dizaine d'années, deux versions sont en production chez des clients et une nouvelle est en cours de développement pour livraison fin d'année, avec une pression importante de la direction pour démontrer une capacité à faire.

Le client a en interne une équipe d'une dizaine de personnes mais cela ne semble pas suffire pour à la fois gérer la maintenance corrective et évolutive, et à la fois développer la nouvelle version d'où le besoin de "renforts". Le client est prêt à travailler avec une équipe située en dehors de ses locaux mais cela n'a rien de très surprenant dans un contrat de TMA : combien de TMA sont elles délocalisées en province ou dans des pays à bas salaires ?

Bref, rien de très excitant...  Mais nous avons un bon contact avec le client, des liens tissés depuis quelques années au travers de plusieurs collaborations, le budget est confortable et nous avons envie d'avancer. Nous faisons donc une contre-proposition : pour le même budget, au lieu d'une équipe pyramidale de 5-6 personnes organisée comme un mini centre de services, nous proposons une équipe répartie auto-organisée de 4 craftspersons expérimentées:

* tous les membres de l'équipe seront des développeurs ou développeuses **chevronnées**, compétentes techniquement mais aussi capables de prendre en main l'organisation et de parler le langage du client ;
* il n'y aura **pas de chef** identifié dans l'équipe ;
* les membres de l'équipe travailleront depuis le lieu de leur choix, **à distance**, ce qui nous permettra de réunir des personnes se connaissant bien et ayant la même vision du travail, où qu'elles se trouvent ;
* l'équipe mettra en oeuvre les pratiques et principes de l'[**eXtreme Programming**](http://extremeprogramming.org), développement dirigé par les tests, intégration continue, boucles de rétroactions courtes, priorisation par la valeur métier…

Après discussion rapide, le client est séduit par la proposition, accepte l'offre et organise un premier rendez-vous afin de nous présenter l'application et l'équipe.

## Mission: Impossible

L'application est structurée en deux composants disjoints : un backend organisé selon le schéma des [pipes and filters](http://www.enterpriseintegrationpatterns.com/patterns/messaging/PipesAndFilters.html) et un frontend de type application web. Entre les deux se trouve une base de données relationnelle d'un éditeur bien connu qui sert de mémoire partagée persistante entre les deux composants.

Première surprise : le backend est codé dans un langage propriétaire multi-paradigmes, à la fois orienté-objets, fonctionnel et même logique, compilé[^1] vers du C++. Au dessus de ce langage socle, l'équipe a développé un autre langage propriétaire, syntaxiquement structuré comme un langage à balises, dans lequel sont codés les processus métiers.

Deuxième surprise : le frontend est certes codé en Java/Javascript/HTML dans une classique _webapp_ JEE, mais il est structuré par un _framework_ maison dont le but est de décrire le flux d'interaction des utilisateurs sous la forme de processus, des graphes dirigés reliant des pages, des requêtes SQL, des traitements...

Troisième surprise (qui n'en est pas vraiment une) : il n'y a aucun test unitaire et très peu de tests automatisés qui couvrent globalement peu de fonctionnalités de l'appplication et uniquement dans le backend. Quelques efforts d'automatisation de tests sur le front ont été entrepris mais ils sont encore balbutiants et ne font pas partie du processus standard de développement.

Nous nous retrouvons donc devant une base de code relativement conséquente, séparée en deux parties développées de manière indépendantes mais fortement couplés par une base de données dont le schéma se complexifie, sans tests, et avec une qualité d'écriture de code plutôt médiocre voire par endroits calamiteuses : copier-coller à outrance, règles de nommages incohérentes, couplage fort entre composants, fonctions et expressions complexes sur plusieurs dizaines de lignes, commentaires obsolètes voire contradictoires...

L'équipe en place suit un processus de développement à la Scrum : stand-up meeting tous les matins, tableaux d'avancement des "user stories" dans Jira, sprints de 2 semaines, scrummaster, niko-niko... Malgré le contexte difficile d'une application vieillissante et rétive à toute modification, le moral et l'ambiance dans l'équipe semblent bons, le processus de développement apparaît plutôt bien suivi.

# Premiers pas

## Prise en main

Nos premiers contacts avec le code du _backend_ sont laborieux, le cycle de développement est long et pénible :
  * il faut tout d'abord écrire le code dans un langage que nous ne maîtrisons pas encore ;
  * le code étant en 32 bits, il nécessite l'installation d'une _toolchain_ et de bibliothèques spécifiques, ainsi que le paramétrage du compilateur C ;
  * ce code doit ensuite être compilé et "packagé" à l'aide scripts et de commandes spécifiques et inhabituelles ;
  * pour exécuter ce code, il faut le déployer selon une structure de répertoire précise qui fait partie des sources ;
  * les scripts et le code de configuration contiennent beaucoup de chemins relatifs de type `../something` ce qui ne facilite pas la compréhension de leur structure ni de leurs effets ;
  * il y a plusieurs étapes manuelles nécessaires pour parvenir à obtenir un système fonctionnel ;
  * la bonne exécution de tout ce processus dépend de variables d'environnements dont la sémantique est floue.

Tout cela prend beaucoup de temps - parfois quelques minutes - et est peu répetable - une installation peut modifier l'environnement de sorte que la prochaine installation échoue). Pour nous qui sommes habitués à un cycle trés rapide (2-3 secondes) pour écrire le code, le compiler, observer un résultat, c'est inacceptable.

Notre premier étape pour prendre en main le système va donc consister à _automatiser_ ce processus au travers de simples scripts de manière à compiler le code, la _packager_ pour produire un paquet déployable, le déployer dans un répertoire, lancer le processus serveur. Lors de l'écriture de ces scripts, nous prenons bien garde à gérer correctement l'arborescence des répertoires : le script devra pouvoir être lancer de n'importe quel endroit et gérer correctement les chemins relatifs existant, ainsi que positionner les variables d'environnement nécessaires. Enfin, nous commencons par travailler sur un nouveau composant ce qui nous permet de mettre au point ce cycle de compilation-déploiement-exécution sans avoir à gérer le passif ni modifier les anciens composants.

Assez rapidement nous atteignons une _cadence_ qui nous permet de mieux comprendre ce qui se passe et partant de commencer à faire se mouvoir notre squelette de composant. Il devient possible de copier/coller du code existant en élaguant les parties non pertinentes et d'obtenir un module fonctionnel, capable de simplement copier ses entrées en sorties mais déployable, compilable, utilisable. L'étape suivante va consister à rajouter de la chair à ce squelette, ce qui nécessite de mettre en places des _tests_ automatisés.

## Sous toute les coutures

Après quelques heures de travail et d'automatisation du cycle de développement, nous comprenons mieux l'architecture du système:

* chaque composant est un _micro-service_ isolable, qui peut se déployer indépendamment des autres, produit à partir de modules - bibliothèques pré-compilées ou directement importées sous forme de sources. Il faudra faire un travail sur les modules qui contiennent beaucoup de code redondant issu de vagues successives de copier-coller à la va-vite mais au moins nous pouvons travailler sur un sous-système isolé, plus simple ;
* chaque composant fonctionne comme un *processeur* relié à d'autres processeurs au moyen de *files* qui peuvent être en entrée ou en sortie ;
* ces files sont de diverses natures mais pour l'essentiel sont constituées de *répertoires* contenant des fichiers aux formats variés et d'une *base de données* ;
* chaque composant produit donc un résultat dans des files de sorties mais aussi des *logs* ;
* le traitement de la base de données comme une *file* en entrée - requêtes `SELECT` - ou en sorties - requêtes `INSERT` ou `UPDATE` - produit un certaine dissonance cognitive d'autant plus qu'il ne s'agit pas ici de [requêtes SQL en continu](https://en.wikipedia.org/wiki/StreamSQL), dissonance qui se constate dans le code où le traitement des requêtes SQL est significativement plus compliqué. Mais il a le mérite d'uniformiser les processus dans un même paradigme ;

Ne maîtrisant pas - encore - le langage source, nous choisissons donc de commencer l'automatisation des tests par le _haut_, c'est-à-dire d'écrire des tests plutôt _systèmes_ qui vont tester le comportement global d'un composant. Cette stratégie nécessite de construire un environnement d'exécution _propre_ pour chaque test, donc d'isoler les données :

* l'arborescence des files d'entrées-sorties est créée automatiquement et de manière unique pour chaque test ;
* la base de données est exécutée dans un container [docker](https://github.com/wnameless/docker-oracle-xe-11g) ce qui nous garantit que chaque exécution est isolée.

## Ceintures et bretelles

Avoir des tests automatiques, repétables, isolés est une étape essentielle dans notre prise en main du système permettant de :

* réduire le temps de cycle de chaque développement et fluidifier le processus en évitant d'inévitables erreurs et oublis qu'introduiraient des étapes manuelles ;
* raccourcir la longueur des boucles de _feedback_ et donc mieux guider notre développement ;
* créer des points de stabilité dans notre travail et pouvoir _commiter_ sur des _barres vertes_.

Ce harnais de test va se révéler fondamental pour nous aider à comprendre le système et surtout le langage et son écosystème.

Se pose alors la question du langage dans lequel écrire ces tests systèmes. Aprés avoir hésité avec Haskell que nous maîtrisons plutôt bien, nous nous tournons vers Python qui présente de nombreux avantages:

* il est disponible facilement et fonctionne sans difficulté sur toutes les plate-formes ;
* il est facile à apprendre et à utiliser ce qui devrait lisser la courbe d'apprentissage pour d'autres personnes qui seraient amenées à rejoindre l'équipe ou travailler sur ces tests ;
* Haskell est un langage puissant mais peu abordable ;
* enfin python est très souple et dispose d'un outillage et d'un éco-système très riche pour gérer les aspects _systèmes_, de manière plus structurée que ne le permettrait le _shell_.

Après quelques itérations nous sommes parvenus à nos fins :

* nous avons une boucle de rétroaction fiable et relativement rapide;
* nous pouvons développer notre code en le dirigeant par des tests plutôt fonctionnels ;
* nous maîtrisons mieux le système et sommes capables de le découper en unités plus petites.

# Évolution des tests

## Réduire l'empreinte carbone

Une fois passée l'euphorie des premières heures et lorsqu'il devient nécessaire de coder réellement des comportements plus fins, nous nous heurtons à une limite attendue de cette stratégie. Les tests python prennent de plus en plus de temps pour gérer la mise en place des données - base, fichiers, leur nettoyage après exécution ou  manipuler des processus qui nécessitent d'utiliser des mécanismes de temporisations - attendre que le processus soit _up_, attendre qu'il soit _down_ - fragiles et lents. Et par ailleurs, nos itérations de codage sont aussi plus longues car chaque test décrit une fonctionnalité de haut niveau qui nécessite plus de travail, donc introduit le risque de faire une erreur qui sera détectée plus tard.

Il est nécessaire de raffiner nos tests, de descendre à un niveau de granularité plus fin et donc d'écrire des tests unitaires _dans le langage d'implémentation_. Les bénéfices attendus sont nombreux :

* cela devrait nous permettre de mieux comprendre le langage que nous manipulons ;
* de s'habituer à écrire du code dans ce langage donc à faire évoluer notre environnement de développement pour l'y adapter ;
* et bien sûr cela sera plus rapide à exécuter.

Notre première étape et d'écrire les tests comme des fonctions dans un exécutable spécifique qui sera _linké_ avec le code fonctionnel : chaque test prend la forme d'une fonciton `testXXX` se terminant par un appel à la fonction interne `assert`, et le `main` exécute l'ensemble des tests de manière séquentielle. Cette première approche nous permet rapidement d'écrire quelques dizaines de tests et de raccourcir encore la boucle de rétroaction.

Dans une deuxième étape, nous sollicitons l'aide d'un architecte du client maîtrisant parfaitement le langage pour nous écrire un framework _XUnit_. Le passage à ce _framework_ de test va réduire la duplication et le travail de "gestion" des tests en découvrant automatiquement les fonctions testables, et clarifier l'écriture des tests grâce à des fonctions _d'assertion_ spécialisées.

Après environ un mois de travail, nous sommes donc dans une situation relativement confortable pour développer sur cette partie _backend_ : nous avons une batterie de tests fonctionnels automatisés qui nous aide dans la réalisation des fonctionnalités attendues du système ; et une batterie de tests unitaires qui nous aide dans les détails de l'écriture du code.

## Refactor Mercilessly

Trois mois plus tard, nous constatons que les tests python sont devenus un _passif_ en tant que tel :

* il y a beaucoup de duplication générée par du copié-collé
* la taille des fichiers de tests qui peut atteindre plus de 1000 lignes les rend ingérables et incompréhensibles ;
* les tests contiennent beaucoup de _technique_ contenant l’automatisation des tâches techniques nécessaires à la mise en place du système ;
* l’intention fonctionnelle de chaque test est noyée dans ce bruit technique.

Une conséquence immédiate de cette complication est que les nouveaux arrivant ont des difficultés à "habiter" le code des tests et à le prendre en main.

### Ce qui se conçoit bien s'énonce clairement

Nous profitons donc de l’arrivée dans l'équipe d'un nouveau membre pour qu’il nous guide vers des tests qui lui _parlent davantage_. Ce travail prend la forme de séances de binômage avec une personne connaissant le système existant et permet de réduire la taille des fonctions de tests en en extrayant des fonctions utilitaires initialisant les états de la base, par exemple `Db_should_contains_a_basket_with_a_discounted_product`.

### Refactoring 2:

**Bernard** ??

### Passage à l'échelle

Malgré ces _refactorings_ une douleur importante persiste : il est difficile de créer un test _from scratch_ pour une nouvelle fonctionnalité, cela oblige à copier/coller beaucoup de code, donc introduit de la duplication et accroît le degré d'entropie du code des tests.

Nous allons donc regrouper toutes nos fonctions utilitaires dans 3 “univers”:

* la base de données:
    * l'insertion de données métier,
    * les assertions sur le contenu de la base ;
* les processus systèmes:
  * démarrage des processus et gestion des états,
  * assertions sur les logs ;
* les fichiers:
  * création des répertoires et manipulation des fichiers,
  * assertions sur les répertoires et fichiers (taille, contenu, nom, structure...).

Ces univers sont plutôt de l'ordre de la technique et non du _métier_. Il s’agit plus de traduire dans notre code et de représenter sous un forme exécutable le _langage_ qui est  utilisé au quotidien dans l’équipe que d'adopter une approche _orienté métier_ ou _domaine_ qui ne nous aide pas à améliorer le système et à rendre les tests plus lisibles. Le _bon_ langage était celui de l’application et de son architecture et non pas celui de l’utilisateur.

## Quid du frontend ?

Parallèlement au travail sur le backend que nous avons détaillé ci-dessus, nous avons été amené à travailler aussi sur le frontend et bien évidemment nous avons voulu mettre en oeuvre la même approche. Mais cette stratégie n'a pas aussi bien fonctionné, malheureusement, bien que l'approche outside-in apparaisse tout aussi pertinente du fait de l'existence là aussi d'un important passif de code.

À cela, nous pouvons identifier plusieurs raisons:

* la nature technique de l’appli: elle est uniquement testable  dans navigateur ce qui implique une complexité de mise en oeuvre bien supérieure, et une fragilité plus importante :
    * c’est clairement (beaucoup) plus compliqué que de poser un fichier texte dans un répertoire et de lancer une commande ;
    * il faut lancer un navigateur, éventuellement dans des containers, avec des résultats non-prédictibles ;
    * la fragilité vient du fait de n'être pas certain d’avoir toujours le même résultat à chaque lancement
* l’investissement initial qui est beaucoup plus élevé avec des risques plus importants. Sur le _backend_, on a pu le faire sans avoir besoin de l’autorisation du sponsor mais sur le _frontend_, le temps de retour sur investissement n’était pas acceptable par le sponsor ;
* nous avons eu au moins un succés mineur : construire un modèle réduit d'application myBatis pour ne pas avoir à  lancer tout le frontend lorsque l'on souhaite travailler uniquement sur les données et requêtes SQL.

# Conclusion

On a réussi parce qu’on ne savait pas que c’était impossible
L'équipe est intéressé pour généraliser notre technique
Approche outside-in -> part des tests haut-niveau/fonctionnel/système puis redescend dans les composants techniques
Ca pourrait etre la graine d’une réécriture dans une techno plus adaptée
Pas besoin d'un framework pour commencer d'écrire des tests U
Contre-intuitif: la partie écrite en Java/Javascript est celle qui nous a le plus posé de problèmes: très difficile à tester, pas de continuité dans l'architecture, empilage de frameworks…

[^1]: Nous découvrirons rapidement que le compilateur ne supporte que le 32 bits et qu'un portage vers du 64 bits demanderait un effort conséquent
